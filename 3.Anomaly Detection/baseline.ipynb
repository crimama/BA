{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn \n",
    "import torch \n",
    "import torchvision \n",
    "import cv2 \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "from PIL import Image\n",
    "from tqdm import tqdm \n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import MNIST\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "import os \n",
    "import yaml \n",
    "import wandb  \n",
    "import pickle \n",
    "\n",
    "from src import Convolution_Auto_Encoder, Mnist_Dataset,MVtecADDataset,Datadir_init\n",
    "from src import MVtecEncoder,MVtecDecoder,Convolution_Auto_Encoder\n",
    "from src import Machine_Metric,Reconstruction_Metric\n",
    "\n",
    "cfg = {'seed':42,\n",
    "       'img_size':256,\n",
    "       'device':'cuda:0',\n",
    "       'encoded_space_dim':256,\n",
    "       'lr':0.001,\n",
    "       'weight_decay':1e-05,\n",
    "       'batch_size':32,\n",
    "       'Epochs':50,\n",
    "       'target_class':6,\n",
    "       'save_dir':'MVtecAD3',\n",
    "       'Dataset_dir':'./Dataset/hazelnut',\n",
    "       'optimizer':'adamw',\n",
    "       'Decription':'Normalize제외 하고 진행'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(cfg,augmentation=None):\n",
    "    #mk save dir \n",
    "    try:\n",
    "        os.mkdir(f\"./Save_models/{cfg['save_dir']}\")\n",
    "    except:\n",
    "        pass\n",
    "    torch.manual_seed(cfg['seed'])\n",
    "    data_dir = cfg['Dataset_dir']\n",
    "    Data_dir = Datadir_init()\n",
    "    train_dirs = Data_dir.train_load()\n",
    "    test_dirs,test_labels = Data_dir.test_load()\n",
    "    indx = int(len(train_dirs)*0.8)\n",
    "\n",
    "    train_dset = MVtecADDataset(cfg,train_dirs[:indx],Augmentation=augmentation)\n",
    "    valid_dset = MVtecADDataset(cfg,train_dirs[indx:],Augmentation=augmentation)\n",
    "    test_dset = MVtecADDataset(cfg,test_dirs,test_labels,Augmentation=augmentation)\n",
    "\n",
    "    train_loader = DataLoader(train_dset,batch_size=cfg['batch_size'],shuffle=True)\n",
    "    valid_loader = DataLoader(valid_dset,batch_size=cfg['batch_size'],shuffle=False)\n",
    "    test_loader = DataLoader(test_dset,batch_size=cfg['batch_size'],shuffle=False)\n",
    "    return train_loader,valid_loader,test_loader \n",
    "\n",
    "def train_epoch(model,dataloader,criterion,optimizer,scheduler,scaler):\n",
    "       model.train()\n",
    "       optimizer.zero_grad()\n",
    "       train_loss = [] \n",
    "       for img,label in dataloader:\n",
    "              img = img.to(cfg['device']).type(torch.float32)\n",
    "              with torch.cuda.amp.autocast():\n",
    "                    y_pred = model(img).type(torch.float32)\n",
    "                    loss = torch.sqrt(criterion(img,y_pred))\n",
    "              #y_pred = model(img).type(torch.float32)\n",
    "              \n",
    "\n",
    "              #Backpropagation\n",
    "              scaler.scale(loss).backward()\n",
    "              scaler.step(optimizer)\n",
    "              scaler.update() \n",
    "              #loss.backward()\n",
    "              #optimizer.step()\n",
    "\n",
    "              #loss save \n",
    "              train_loss.append(loss.detach().cpu().numpy())\n",
    "       scheduler.step() \n",
    "       print(f'\\t epoch : {epoch+1} train loss : {np.mean(train_loss):.3f}')\n",
    "       return np.mean(train_loss)\n",
    "\n",
    "def valid_epoch(model,dataloader,criterion):\n",
    "       model.eval()\n",
    "       valid_loss = [] \n",
    "       with torch.no_grad():\n",
    "              for img,label in dataloader:\n",
    "                     img = img.to(cfg['device'])\n",
    "                     y_pred = model(img)\n",
    "                     loss = torch.sqrt(criterion(y_pred,img))\n",
    "                     valid_loss.append(loss.detach().cpu().numpy())\n",
    "       print(f'\\t epoch : {epoch+1} valid loss : {np.mean(valid_loss):.3f}')\n",
    "       fig, (ax1,ax2) = plt.subplots(ncols=2,nrows=1,figsize=(5, 5))\n",
    "       ax1.imshow(img[0].detach().cpu().permute(1,2,0).numpy())\n",
    "       ax2.imshow(y_pred[0].detach().cpu().permute(1,2,0).numpy())\n",
    "       plt.show()\n",
    "       return np.mean(valid_loss)   \n",
    "\n",
    "def Save_resuld(cfg):\n",
    "    metric =  {} \n",
    "    metric['auto'] = {} \n",
    "    metric['machine']={}\n",
    "\n",
    "    cfg = yaml.load(open('./Save_models/MVtecAD4/config.yaml','r'), Loader=yaml.FullLoader)\n",
    "    machine = Machine_Metric(cfg)\n",
    "    auto = Reconstruction_Metric(cfg)\n",
    "\n",
    "    acc,pre,rec,f1 = machine.main()\n",
    "    [AUROC,ROC], [ACC,PRE,RECALL,F1] = auto.main() \n",
    "\n",
    "    metric['auto']['auroc'] = AUROC\n",
    "    metric['auto']['roc'] = ROC \n",
    "    metric['auto']['metric'] =[ACC,PRE,RECALL,F1]\n",
    "    metric['machine']['metric']=[acc,pre,rec,f1]\n",
    "\n",
    "    with open(f\"./Save_models/{cfg['save_dir']}/temp.pickle\",'wb') as fw:\n",
    "        pickle.dump(metric, fw)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_loader, valid_normal_loader,valid_anomal_loader  = preprocess(cfg)\n",
    "train_loader,valid_loader,test_loader   = preprocess(cfg)\n",
    "model = Convolution_Auto_Encoder(MVtecEncoder,MVtecDecoder,cfg['encoded_space_dim']).to(cfg['device'])\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(),lr=cfg['lr'],weight_decay=cfg['weight_decay'])\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer,T_max=100,eta_min=0)\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "\n",
    "total_train_loss = [] \n",
    "total_valid_loss = [] \n",
    "best_valid_loss = np.inf \n",
    "\n",
    "for epoch in tqdm(range(cfg['Epochs'])):\n",
    "    train_loss = train_epoch(model,train_loader,criterion,optimizer,scheduler,scaler)\n",
    "    valid_loss = valid_epoch(model,valid_loader,criterion)\n",
    "    total_train_loss.append(train_loss)\n",
    "    total_valid_loss.append(valid_loss)\n",
    "\n",
    "    if valid_loss < best_valid_loss:\n",
    "        torch.save(model,f\"./Save_models/{cfg['save_dir']}/best.pt\")\n",
    "        best_valid_loss = valid_loss \n",
    "        print(f'\\t Model save : {epoch} | best loss : {best_valid_loss :.3f}')\n",
    "        cfg['save_epoch'] = epoch \n",
    "        cfg['best_valid_loss'] = best_valid_loss\n",
    "    \n",
    "    if valid_loss != valid_loss:\n",
    "        model = torch.load(f\"./Save_models/{cfg['save_dir']}/best.pt\")\n",
    "        print('Model rewinded')\n",
    "cfg['last_train_loss'] = total_train_loss[-1]\n",
    "cfg['last_valid_loss'] = total_valid_loss[-1]\n",
    "\n",
    "f = open(f\"./Save_models/{cfg['save_dir']}/config.yaml\",'w+')\n",
    "yaml.dump(cfg, f, allow_unicode=True)\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
