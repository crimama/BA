{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19c73783-b040-4c48-a336-eb73d9886f8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pickle \n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import os \n",
    "from glob import glob \n",
    "import tqdm \n",
    "from PIL import Image \n",
    "from tqdm import tqdm \n",
    "import wandb\n",
    "import math\n",
    "\n",
    "from sklearn.metrics import f1_score,accuracy_score\n",
    "\n",
    "from src.Dataset import CifarDataset,label_unlabel_load,dataset_load\n",
    "from src.Models import Model,PiModel\n",
    "from src.Loss import PiCriterion\n",
    "\n",
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F \n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3f49680-0d9d-4180-b365-04bb367ecce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_transform():\n",
    "    color_jitter = transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.1)\n",
    "    transformer = transforms.Compose([\n",
    "                                          transforms.RandomApply([color_jitter],p=0.8),\n",
    "                                          transforms.RandomResizedCrop(32),\n",
    "                                          transforms.GaussianBlur(kernel_size=int(0.1*32))\n",
    "                                         ])\n",
    "    return transformer\n",
    " \n",
    "def make_valid(dataset = 'cifar10'):\n",
    "    (train_imgs,train_labels),(test_imgs,test_labels) = dataset_load(dataset)\n",
    "    idx = np.random.choice(np.arange(len(train_imgs)),5000,replace=False)\n",
    "    valid_set = {'imgs':train_imgs[idx],\n",
    "        'labels':train_labels[idx]}\n",
    "    return valid_set \n",
    "\n",
    "def train(model,criterion,optimizer,train_loader,cfg,transformer):\n",
    "    global epoch \n",
    "    model.train() \n",
    "    tl_loss = [] \n",
    "    tu_loss = [] \n",
    "    total_loss = [] \n",
    "    for batch_img,batch_labels in tqdm(train_loader):\n",
    "        \n",
    "        batch_img_1 = transformer(batch_img.type(torch.float32).to(cfg['device']))\n",
    "        batch_img_2 = transformer(batch_img.type(torch.float32).to(cfg['device']))\n",
    "        batch_labels = batch_labels.to(cfg['device'])\n",
    "        \n",
    "        y_pred_1 = model(batch_img_1,True)\n",
    "        y_pred_2 = model(batch_img_2,True)\n",
    "        loss,tl,tu,weight = criterion(y_pred_1,y_pred_2,batch_labels,epoch)\n",
    "        \n",
    "        total_loss.append(loss.detach().cpu().numpy())\n",
    "        tl_loss.append(tl.detach().cpu().numpy())\n",
    "        tu_loss.append(tu.detach().cpu().numpy())\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    return np.mean(total_loss),np.mean(tl_loss),np.mean(tu_loss),weight\n",
    "\n",
    "def valid(model,test_loader,cfg):\n",
    "    labels = []\n",
    "    y_preds = [] \n",
    "    model.eval() \n",
    "    for batch_imgs,batch_labels in test_loader:\n",
    "        batch_imgs = batch_imgs.type(torch.float32).to(cfg['device'])\n",
    "        with torch.no_grad():\n",
    "            y_pred = model(batch_imgs,False)\n",
    "        y_pred = torch.argmax(F.softmax(y_pred),dim=1)\n",
    "        y_pred = y_pred.detach().cpu().numpy()\n",
    "        \n",
    "        y_preds.extend(y_pred)\n",
    "        labels.extend(batch_labels.detach().cpu().numpy())    \n",
    "    f1 = f1_score(np.array(y_preds),np.array(labels),average='macro')\n",
    "    auc = accuracy_score(np.array(y_preds),np.array(labels))\n",
    "    return f1, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b77e991",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = {}\n",
    "cfg['dataset'] = 'cifar10'\n",
    "cfg['model_name'] = 'resnet18'\n",
    "cfg['unlabel_ratio'] = 0.6\n",
    "cfg['batch_size'] = 100 \n",
    "cfg['device'] = 'cuda:0'\n",
    "cfg['lr'] = 0.003 \n",
    "cfg['beta1'] = 0.8\n",
    "cfg['beta2'] = 0.999 \n",
    "cfg['epochs'] = 300 \n",
    "cfg['std'] = 0.15 \n",
    "cfg['super_only'] = False\n",
    "\n",
    "\n",
    "\n",
    "train_set,test_set = label_unlabel_load(cfg)\n",
    "valid_set = make_valid()\n",
    "\n",
    "train_dataset = CifarDataset(train_set, unlabel=False)\n",
    "valid_dataset = CifarDataset(valid_set, unlabel=False)\n",
    "test_dataset  = CifarDataset(test_set,unlabel=False)\n",
    "\n",
    "train_loader = DataLoader(train_dataset,batch_size=cfg['batch_size'],shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset,batch_size=cfg['batch_size'],shuffle=False)\n",
    "test_loader  = DataLoader(test_dataset,batch_size=cfg['batch_size'],shuffle=False)\n",
    "\n",
    "transformer = make_transform()\n",
    "\n",
    "#model = PiModel(device='cuda')\n",
    "model = Model(cfg['model_name']).to('cuda')\n",
    "criterion = PiCriterion(cfg)\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=cfg['lr'],betas=(cfg['beta1'],cfg['beta2']))\n",
    "\n",
    "\n",
    "\n",
    "best_epoch = np.inf \n",
    "for epoch in range(cfg['epochs']):\n",
    "    loss,tl_loss,tu_loss,weight =  train(model,criterion,optimizer,train_loader,cfg,transformer)\n",
    "    f1 , auc = valid(model,valid_loader,cfg)\n",
    "    print(f'\\n Epochs : {epoch}')\n",
    "    print(f'\\n loss : {loss} | tl_loss : {tl_loss} | tu_loss : {tu_loss}')\n",
    "    print(f'\\n valid f1 : {f1}')\n",
    "    print(f'\\n valid auc : {auc}')\n",
    "    \n",
    "    if loss < best_epoch:\n",
    "        torch.save(model,'./Save_models/best.pt')\n",
    "        best_epoch = loss \n",
    "        print(f'model saved | best loss :{best_epoch}')\n",
    "    '''\n",
    "    wandb.log({'loss':loss,\n",
    "               'tl_loss':tl_loss,\n",
    "               'tu_loss':tu_loss,\n",
    "               'weight':weight\n",
    "               })\n",
    "    '''\n",
    "    #if  loss > 10000:\n",
    "     #   model = torch.load('./Save_models/best.pt')\n",
    "      #  print('Model reloaded')\n",
    "f1 , auc = valid(model,test_loader,transformer,cfg)\n",
    "print(f\"\\n F1 score : {f1} | Auccuracy :\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "24a32428",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CallBack:\n",
    "    def __init__(self,cfg):\n",
    "        self.best_loss = np.inf\n",
    "        self.best_epoch = 0\n",
    "        self.cfg = cfg \n",
    "    \n",
    "    def model_checkpoint(self,epoch,loss):\n",
    "        torch.save(model,f\"./Save_models/{self.cfg['dir']}/best.pt\")\n",
    "        self.best_loss = loss \n",
    "        self.best_epoch = epoch\n",
    "        print(f'model saved | best loss :{best_epoch}')\n",
    "        \n",
    "    def model_reloaded(self):\n",
    "        model = torch.load(f\"./Save_models/{self.cfg['dir']}/best.pt\")\n",
    "        print('Model reloaded')\n",
    "        return model \n",
    "    \n",
    "    def model_log(self,result_log):\n",
    "        wandb.log(result_log)\n",
    "        \n",
    "        \n",
    "    def __call__(self,epoch,result_log):\n",
    "        #log \n",
    "        loss = result_log['loss']\n",
    "        self.model_log(result_log)\n",
    "        #check point \n",
    "        if loss < best_epoch:\n",
    "            self.model_checkpoint(epoch,loss)\n",
    "        #reloaded \n",
    "        if loss > 100000:\n",
    "            return self.model_reloaded()\n",
    "        #early stopping \n",
    "        if epoch + 15 > self.best_epoch:\n",
    "            return True "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1ed461df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/500 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "for batch_img,batch_labels in tqdm(train_loader):\n",
    "    \n",
    "    batch_img_1 = transformer(batch_img.type(torch.float32).to(cfg['device']))\n",
    "    batch_img_2 = transformer(batch_img.type(torch.float32).to(cfg['device']))\n",
    "    batch_labels = batch_labels.to(cfg['device'])\n",
    "    \n",
    "    y_pred_1 = model(batch_img_1,True)\n",
    "    y_pred_2 = model(batch_img_2,True)\n",
    "    loss,tl,tu,weight = criterion(y_pred_1,y_pred_2,batch_labels,epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a51543ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 9, 9, 5, 6, 4, 6, 1, 6, 6, 2, 4, 8, 2, 2, 8, 9, 5, 9, 1, 7, 4, 4, 8,\n",
       "        0, 9, 0, 6, 1, 5, 0, 9, 7, 5, 8, 8, 8, 1], device='cuda:0')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_idx = (batch_labels!=-1).nonzero().flatten()\n",
    "batch_labels[label_idx].shap"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10 (default, Mar 15 2022, 12:22:08) \n[GCC 9.4.0]"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
