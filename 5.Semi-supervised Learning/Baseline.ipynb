{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19c73783-b040-4c48-a336-eb73d9886f8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pickle \n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import os \n",
    "from glob import glob \n",
    "import tqdm \n",
    "from PIL import Image \n",
    "from tqdm import tqdm \n",
    "import wandb\n",
    "import math\n",
    "\n",
    "from sklearn.metrics import f1_score,accuracy_score\n",
    "\n",
    "from src.Dataset import CifarDataset,label_unlabel_load,dataset_load\n",
    "from src.Models import Model,PiModel\n",
    "from src.Loss import PiCriterion\n",
    "\n",
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F \n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a3f49680-0d9d-4180-b365-04bb367ecce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_transform():\n",
    "    color_jitter = transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.1)\n",
    "    transformer = transforms.Compose([\n",
    "                                          transforms.RandomApply([color_jitter],p=0.8),\n",
    "                                          transforms.RandomResizedCrop(32),\n",
    "                                          transforms.GaussianBlur(kernel_size=int(0.1*32))\n",
    "                                         ])\n",
    "    return transformer\n",
    " \n",
    "def make_valid(dataset = 'cifar10'):\n",
    "    (train_imgs,train_labels),(test_imgs,test_labels) = dataset_load(dataset)\n",
    "    idx = np.random.choice(np.arange(len(train_imgs)),5000,replace=False)\n",
    "    valid_set = {'imgs':train_imgs[idx],\n",
    "        'labels':train_labels[idx]}\n",
    "    return valid_set \n",
    "\n",
    "def train(model,criterion,optimizer,train_loader,cfg,transformer):\n",
    "    global epoch \n",
    "    model.train() \n",
    "    tl_loss = [] \n",
    "    tu_loss = [] \n",
    "    total_loss = [] \n",
    "    for batch_img,batch_labels in tqdm(train_loader):\n",
    "        \n",
    "        batch_img_1 = transformer(batch_img.type(torch.float32).to(cfg['device']))\n",
    "        batch_img_2 = transformer(batch_img.type(torch.float32).to(cfg['device']))\n",
    "        batch_labels = batch_labels.to(cfg['device'])\n",
    "        \n",
    "        y_pred_1 = model(batch_img_1,True)\n",
    "        y_pred_2 = model(batch_img_2,True)\n",
    "        loss,tl,tu,weight = criterion(y_pred_1,y_pred_2,batch_labels,epoch)\n",
    "        \n",
    "        total_loss.append(loss.detach().cpu().numpy())\n",
    "        tl_loss.append(tl.detach().cpu().numpy())\n",
    "        tu_loss.append(tu.detach().cpu().numpy())\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    return np.mean(total_loss),np.mean(tl_loss),np.mean(tu_loss),weight\n",
    "\n",
    "def valid(model,test_loader,cfg):\n",
    "    labels = []\n",
    "    y_preds = [] \n",
    "    model.eval() \n",
    "    for batch_imgs,batch_labels in test_loader:\n",
    "        batch_imgs = batch_imgs.type(torch.float32).to(cfg['device'])\n",
    "        with torch.no_grad():\n",
    "            y_pred = model(batch_imgs,False)\n",
    "        y_pred = torch.argmax(F.softmax(y_pred),dim=1)\n",
    "        y_pred = y_pred.detach().cpu().numpy()\n",
    "        \n",
    "        y_preds.extend(y_pred)\n",
    "        labels.extend(batch_labels.detach().cpu().numpy())    \n",
    "    f1 = f1_score(np.array(y_preds),np.array(labels),average='macro')\n",
    "    auc = accuracy_score(np.array(y_preds),np.array(labels))\n",
    "    return f1, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d695e48e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcrimama-\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data/대학원수업/BA/5.Semi-supervised Learning/wandb/run-20221224_054541-2iax0jos</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/crimama-/BA_SSL/runs/2iax0jos\" target=\"_blank\">Try2</a></strong> to <a href=\"https://wandb.ai/crimama-/BA_SSL\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/crimama-/BA_SSL/runs/2iax0jos?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f26b97edd90>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(project='BA_SSL',\n",
    "           name = 'Try2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b77e991",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:17<00:00, 28.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epochs : 0\n",
      "\n",
      " loss : 1.9154711961746216 | tl_loss : 1.9154711961746216 | tu_loss : 0.0\n",
      "\n",
      " valid f1 : 0.2952153543952718\n",
      "\n",
      " valid auc : 0.3502\n",
      "model saved | best loss :1.9154711961746216\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:17<00:00, 28.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epochs : 1\n",
      "\n",
      " loss : 1.7168867588043213 | tl_loss : 1.6206587553024292 | tu_loss : 0.09622790664434433\n",
      "\n",
      " valid f1 : 0.3855233803068166\n",
      "\n",
      " valid auc : 0.445\n",
      "model saved | best loss :1.7168867588043213\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:17<00:00, 28.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epochs : 2\n",
      "\n",
      " loss : 1.5330302715301514 | tl_loss : 1.4223341941833496 | tu_loss : 0.110695980489254\n",
      "\n",
      " valid f1 : 0.4793518422329397\n",
      "\n",
      " valid auc : 0.52\n",
      "model saved | best loss :1.5330302715301514\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:17<00:00, 28.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epochs : 3\n",
      "\n",
      " loss : 1.4351662397384644 | tl_loss : 1.3105820417404175 | tu_loss : 0.12458419799804688\n",
      "\n",
      " valid f1 : 0.38286537821779826\n",
      "\n",
      " valid auc : 0.4292\n",
      "model saved | best loss :1.4351662397384644\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:17<00:00, 28.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epochs : 4\n",
      "\n",
      " loss : 1.407201886177063 | tl_loss : 1.2669321298599243 | tu_loss : 0.14026974141597748\n",
      "\n",
      " valid f1 : 0.44498737115896453\n",
      "\n",
      " valid auc : 0.4958\n",
      "model saved | best loss :1.407201886177063\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:17<00:00, 27.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epochs : 5\n",
      "\n",
      " loss : 1.372666597366333 | tl_loss : 1.218932867050171 | tu_loss : 0.1537337750196457\n",
      "\n",
      " valid f1 : 0.5330347321997548\n",
      "\n",
      " valid auc : 0.568\n",
      "model saved | best loss :1.372666597366333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:17<00:00, 28.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epochs : 6\n",
      "\n",
      " loss : 1.3100656270980835 | tl_loss : 1.1463325023651123 | tu_loss : 0.16373297572135925\n",
      "\n",
      " valid f1 : 0.6239629592958174\n",
      "\n",
      " valid auc : 0.642\n",
      "model saved | best loss :1.3100656270980835\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:17<00:00, 28.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epochs : 7\n",
      "\n",
      " loss : 1.3090547323226929 | tl_loss : 1.137319803237915 | tu_loss : 0.1717347949743271\n",
      "\n",
      " valid f1 : 0.5935173103555849\n",
      "\n",
      " valid auc : 0.6152\n",
      "model saved | best loss :1.3090547323226929\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:17<00:00, 27.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epochs : 8\n",
      "\n",
      " loss : 1.2967170476913452 | tl_loss : 1.1098322868347168 | tu_loss : 0.18688462674617767\n",
      "\n",
      " valid f1 : 0.6120155710460701\n",
      "\n",
      " valid auc : 0.6306\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [6], line 45\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m valid auc : \u001b[39m\u001b[39m{\u001b[39;00mauc\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m     44\u001b[0m \u001b[39mif\u001b[39;00m loss \u001b[39m<\u001b[39m best_epoch:\n\u001b[0;32m---> 45\u001b[0m     torch\u001b[39m.\u001b[39;49msave(model,\u001b[39m'\u001b[39;49m\u001b[39m./Save_models/best.pt\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m     46\u001b[0m     best_epoch \u001b[39m=\u001b[39m loss \n\u001b[1;32m     47\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmodel saved | best loss :\u001b[39m\u001b[39m{\u001b[39;00mbest_epoch\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/serialization.py:379\u001b[0m, in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[1;32m    377\u001b[0m \u001b[39mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[1;32m    378\u001b[0m     \u001b[39mwith\u001b[39;00m _open_zipfile_writer(opened_file) \u001b[39mas\u001b[39;00m opened_zipfile:\n\u001b[0;32m--> 379\u001b[0m         _save(obj, opened_zipfile, pickle_module, pickle_protocol)\n\u001b[1;32m    380\u001b[0m         \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m    381\u001b[0m _legacy_save(obj, opened_file, pickle_module, pickle_protocol)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/serialization.py:601\u001b[0m, in \u001b[0;36m_save\u001b[0;34m(obj, zip_file, pickle_module, pickle_protocol)\u001b[0m\n\u001b[1;32m    597\u001b[0m \u001b[39m# given that we copy things around anyway, we might use storage.cpu()\u001b[39;00m\n\u001b[1;32m    598\u001b[0m \u001b[39m# this means to that to get tensors serialized, you need to implement\u001b[39;00m\n\u001b[1;32m    599\u001b[0m \u001b[39m# .cpu() on the underlying Storage\u001b[39;00m\n\u001b[1;32m    600\u001b[0m \u001b[39mif\u001b[39;00m storage\u001b[39m.\u001b[39mdevice\u001b[39m.\u001b[39mtype \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m--> 601\u001b[0m     storage \u001b[39m=\u001b[39m storage\u001b[39m.\u001b[39;49mcpu()\n\u001b[1;32m    602\u001b[0m \u001b[39m# Now that it is on the CPU we can directly copy it into the zip file\u001b[39;00m\n\u001b[1;32m    603\u001b[0m num_bytes \u001b[39m=\u001b[39m storage\u001b[39m.\u001b[39mnbytes()\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/storage.py:111\u001b[0m, in \u001b[0;36m_StorageBase.cpu\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[39m\"\"\"Returns a CPU copy of this storage if it's not already on the CPU\"\"\"\u001b[39;00m\n\u001b[1;32m    110\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice\u001b[39m.\u001b[39mtype \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m--> 111\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_UntypedStorage(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msize())\u001b[39m.\u001b[39;49mcopy_(\u001b[39mself\u001b[39;49m, \u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m    112\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    113\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "cfg = {}\n",
    "cfg['dataset'] = 'cifar10'\n",
    "cfg['model_name'] = 'resnet18'\n",
    "cfg['unlabel_ratio'] = 0\n",
    "cfg['batch_size'] = 100 \n",
    "cfg['device'] = 'cuda:0'\n",
    "cfg['lr'] = 0.003 \n",
    "cfg['beta1'] = 0.8\n",
    "cfg['beta2'] = 0.999 \n",
    "cfg['epochs'] = 300 \n",
    "cfg['std'] = 0.15 \n",
    "\n",
    "\n",
    "\n",
    "train_set,test_set = label_unlabel_load(cfg)\n",
    "valid_set = make_valid()\n",
    "\n",
    "train_dataset = CifarDataset(train_set, unlabel=False)\n",
    "valid_dataset = CifarDataset(valid_set, unlabel=False)\n",
    "test_dataset  = CifarDataset(test_set,unlabel=False)\n",
    "\n",
    "train_loader = DataLoader(train_dataset,batch_size=cfg['batch_size'],shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset,batch_size=cfg['batch_size'],shuffle=False)\n",
    "test_loader  = DataLoader(test_dataset,batch_size=cfg['batch_size'],shuffle=False)\n",
    "\n",
    "transformer = make_transform()\n",
    "\n",
    "#model = PiModel(device='cuda')\n",
    "model = Model(cfg['model_name']).to('cuda')\n",
    "criterion = PiCriterion(cfg)\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=cfg['lr'],betas=(cfg['beta1'],cfg['beta2']))\n",
    "\n",
    "\n",
    "\n",
    "best_epoch = np.inf \n",
    "for epoch in range(cfg['epochs']):\n",
    "    loss,tl_loss,tu_loss,weight =  train(model,criterion,optimizer,train_loader,cfg,transformer)\n",
    "    f1 , auc = valid(model,valid_loader,cfg)\n",
    "    print(f'\\n Epochs : {epoch}')\n",
    "    print(f'\\n loss : {loss} | tl_loss : {tl_loss} | tu_loss : {tu_loss}')\n",
    "    print(f'\\n valid f1 : {f1}')\n",
    "    print(f'\\n valid auc : {auc}')\n",
    "    \n",
    "    if loss < best_epoch:\n",
    "        torch.save(model,'./Save_models/best.pt')\n",
    "        best_epoch = loss \n",
    "        print(f'model saved | best loss :{best_epoch}')\n",
    "    wandb.log({'loss':loss,\n",
    "               'tl_loss':tl_loss,\n",
    "               'tu_loss':tu_loss,\n",
    "               'weight':weight\n",
    "               })\n",
    "    \n",
    "    #if  loss > 10000:\n",
    "     #   model = torch.load('./Save_models/best.pt')\n",
    "      #  print('Model reloaded')\n",
    "f1 , auc = valid(model,test_loader,transformer,cfg)\n",
    "print(f\"\\n F1 score : {f1} | Auccuracy :\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
