{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import os \n",
    "from glob import glob \n",
    "import tqdm \n",
    "from PIL import Image \n",
    "from tqdm import tqdm \n",
    "import wandb\n",
    "import math\n",
    "\n",
    "from sklearn.metrics import f1_score,accuracy_score\n",
    "\n",
    "from src.Dataset import CifarDataset,label_unlabel_load,dataset_load\n",
    "from src.Models import Model,PiModel\n",
    "from src.Loss import PiCriterion\n",
    "\n",
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F \n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset \n",
    "from torchvision import transforms\n",
    "import timm \n",
    "\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터셋 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def from_pickle_to_img(file,name):\n",
    "    with open(file, 'rb') as f:\n",
    "        data = pickle.load(f,encoding='bytes')\n",
    "    if name == 'cifar10':\n",
    "        #해당 데이터의 경우 flatten한 상태로 저장이 되어 있기 때문에 이를 image에 맞춰서 변형해줌 \n",
    "        batch_imgs = data[b'data'].reshape(-1,3,32,32).transpose(0,2,3,1) \n",
    "        batch_labels = data[b'labels']\n",
    "        return batch_imgs, np.array(batch_labels) \n",
    "    \n",
    "    elif name == 'cifar100':\n",
    "        batch_imgs = data[b'data'].reshape(-1,32,32,3)\n",
    "        batch_labels = data[b'fine_labels']\n",
    "        return batch_imgs, np.array(batch_labels) \n",
    "\n",
    "def load_cifar10():\n",
    "    files = sorted(glob('./Dataset/cifar-10-batches-py/*')[1:-1])\n",
    "    imgs = [] \n",
    "    labels = [] \n",
    "    for file in files:\n",
    "        batch_imgs,batch_labels = from_pickle_to_img(file,'cifar10')\n",
    "        imgs.extend(batch_imgs)\n",
    "        labels.extend(batch_labels)\n",
    "    labels = np.array(labels)\n",
    "    imgs = np.array(imgs)\n",
    "    return (imgs[:50000],labels[:50000]),(imgs[50000:],labels[50000:])\n",
    "\n",
    "def load_cifar100():\n",
    "    files = sorted(glob('./Dataset/cifar-100-python/*'))[-2:]\n",
    "    train_imgs,train_labels = from_pickle_to_img(files[1],'cifar100')\n",
    "    test_imgs,test_labels = from_pickle_to_img(files[0],'cifar100')\n",
    "    return (train_imgs,train_labels),(test_imgs,test_labels)\n",
    "\n",
    "#데이터셋 로드 후 label - unlabel 데이터 만드는 메소드 \n",
    "def label_unlabel_load(cfg):\n",
    "    (train_imgs,train_labels),(test_imgs,test_labels) = dataset_load(cfg['dataset'])\n",
    "    labels = np.unique(train_labels)\n",
    "    label = labels[0]\n",
    "    for label in labels:\n",
    "        label_idx = (train_labels ==label).nonzero()[0]\n",
    "        unlabel_idx = np.random.choice(label_idx,int(len(label_idx)*cfg['unlabel_ratio']),replace=False)\n",
    "        train_labels[unlabel_idx] = -1 \n",
    "        \n",
    "        \n",
    "    train = {'imgs':train_imgs,\n",
    "            'labels':train_labels}\n",
    "    test = {'imgs':test_imgs,\n",
    "            'labels':test_labels}\n",
    "    return train, test \n",
    "\n",
    "class CifarDataset(Dataset):\n",
    "    def __init__(self,data,unlabel=False,transform=None):\n",
    "        super(CifarDataset,self).__init__()\n",
    "        self.transform = transform \n",
    "        self.imgs = data['imgs']\n",
    "        self.labels = data['labels']\n",
    "        self.unlabel = unlabel \n",
    "        self.transform = self.transfrom_init(transform)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n",
    "    \n",
    "    def transfrom_init(self,transform):\n",
    "        if transform == None:\n",
    "            return transforms.Compose([transforms.ToTensor()])\n",
    "        else:\n",
    "            return transform \n",
    "\n",
    "            \n",
    "    def __getitem__(self,idx):\n",
    "        if self.unlabel:\n",
    "            img = self.transform(self.imgs[idx])\n",
    "            return img\n",
    "        else:\n",
    "            img = self.transform(self.imgs[idx])\n",
    "            label = self.labels[idx]\n",
    "            return img,label "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 학습 모듈 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_transform():\n",
    "    color_jitter = transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.1)\n",
    "    transformer = transforms.Compose([\n",
    "                                          transforms.RandomApply([color_jitter],p=0.8),\n",
    "                                          transforms.RandomResizedCrop(32),\n",
    "                                          transforms.GaussianBlur(kernel_size=int(0.1*32))\n",
    "                                         ])\n",
    "    return transformer\n",
    " \n",
    "def make_valid(dataset = 'cifar10'):\n",
    "    (train_imgs,train_labels),(test_imgs,test_labels) = dataset_load(dataset)\n",
    "    idx = np.random.choice(np.arange(len(train_imgs)),5000,replace=False)\n",
    "    valid_set = {'imgs':train_imgs[idx],\n",
    "        'labels':train_labels[idx]}\n",
    "    return valid_set \n",
    "\n",
    "def train(model,criterion,optimizer,train_loader,cfg,transformer):\n",
    "    global epoch \n",
    "    model.train() \n",
    "    tl_loss = [] \n",
    "    tu_loss = [] \n",
    "    total_loss = [] \n",
    "    for batch_img,batch_labels in tqdm(train_loader):\n",
    "        \n",
    "        batch_img_1 = transformer(batch_img.type(torch.float32).to(cfg['device']))\n",
    "        batch_img_2 = transformer(batch_img.type(torch.float32).to(cfg['device']))\n",
    "        batch_labels = batch_labels.to(cfg['device'])\n",
    "        \n",
    "        y_pred_1 = model(batch_img_1,True)\n",
    "        y_pred_2 = model(batch_img_2,True)\n",
    "        loss,tl,tu,weight = criterion(y_pred_1,y_pred_2,batch_labels,epoch)\n",
    "        \n",
    "        total_loss.append(loss.detach().cpu().numpy())\n",
    "        tl_loss.append(tl.detach().cpu().numpy())\n",
    "        tu_loss.append(tu.detach().cpu().numpy())\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    return np.mean(total_loss),np.mean(tl_loss),np.mean(tu_loss),weight\n",
    "\n",
    "def valid(model,test_loader,cfg):\n",
    "    labels = []\n",
    "    y_preds = [] \n",
    "    model.eval() \n",
    "    for batch_imgs,batch_labels in test_loader:\n",
    "        batch_imgs = batch_imgs.type(torch.float32).to(cfg['device'])\n",
    "        with torch.no_grad():\n",
    "            y_pred = model(batch_imgs,False)\n",
    "        y_pred = torch.argmax(F.softmax(y_pred),dim=1)\n",
    "        y_pred = y_pred.detach().cpu().numpy()\n",
    "        \n",
    "        y_preds.extend(y_pred)\n",
    "        labels.extend(batch_labels.detach().cpu().numpy())    \n",
    "    f1 = f1_score(np.array(y_preds),np.array(labels),average='macro')\n",
    "    auc = accuracy_score(np.array(y_preds),np.array(labels))\n",
    "    return f1, auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self,model_name='ssl_resnet50',dataset_name='cifar10'):\n",
    "        super(Model,self).__init__()\n",
    "        self.model_name = model_name \n",
    "        self.encoder = self.pretrained_encoder(model_name)\n",
    "        self.linear = self.output_layer(dataset_name)\n",
    "        \n",
    "    \n",
    "    def pretrained_encoder(self,model_name):\n",
    "        res = timm.create_model(model_name,pretrained=True)\n",
    "        encoder = nn.Sequential(*(list(res.children())[:-1]))\n",
    "        return encoder \n",
    "    \n",
    "    def output_layer(self,dataset_name):\n",
    "        in_features = list(self.encoder[-2][-1].children())[-3].out_channels\n",
    "        if dataset_name == 'cifar10':\n",
    "            return nn.Linear(in_features = in_features,out_features= 10)\n",
    "        else:\n",
    "            return nn.Linear(in_features = in_features,out_features= 100)\n",
    "        \n",
    "    def forward(self,x,_):\n",
    "        x = self.encoder(x)\n",
    "        x = self.linear(x)\n",
    "        return x \n",
    "    \n",
    "class GaussianNoise(nn.Module):\n",
    "    \n",
    "    def __init__(self, batch_size, input_shape=(1, 32, 32), std=0.05,device='cpu'):\n",
    "        super(GaussianNoise, self).__init__()\n",
    "        self.shape = (batch_size,) + input_shape\n",
    "        self.noise = Variable(torch.zeros(self.shape)).to(device)\n",
    "        self.std = std\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.noise.data.normal_(0, std=self.std)\n",
    "        return x + self.noise\n",
    "    \n",
    "    \n",
    "class PiModel(nn.Module):\n",
    "    def __init__(self,num_labels=10,batch_size=100,std=0.15,device='cpu'):\n",
    "        super(PiModel,self).__init__()\n",
    "        self.noise = GaussianNoise(batch_size,std=std,device=device)\n",
    "        self.conv1 = self.conv_block(3,128).to(device)\n",
    "        self.conv2 = self.conv_block(128,256).to(device)\n",
    "        self.conv3 = self.conv3_block().to(device)\n",
    "        self.linear = nn.Linear(128,num_labels).to(device)\n",
    "        \n",
    "        \n",
    "    def conv_block(self,input_channel,num_filters):\n",
    "        return nn.Sequential(\n",
    "                                nn.Conv2d(input_channel,num_filters,3,1,1),\n",
    "                                nn.LeakyReLU(0.1),\n",
    "                                nn.Conv2d(num_filters,num_filters,3,1,1),\n",
    "                                nn.LeakyReLU(0.1),\n",
    "                                nn.Conv2d(num_filters,num_filters,3,1,1),\n",
    "                                nn.LeakyReLU(0.1),\n",
    "                                nn.MaxPool2d(2,2),\n",
    "                                nn.Dropout(0.5)                    \n",
    "                             )\n",
    "    def conv3_block (self):\n",
    "        return nn.Sequential(\n",
    "                              nn.Conv2d(256,512,3,1,0),\n",
    "                              nn.LeakyReLU(0.1),\n",
    "                              nn.Conv2d(512,256,1,1),\n",
    "                              nn.LeakyReLU(0.1),\n",
    "                              nn.Conv2d(256,128,1,1),\n",
    "                              nn.LeakyReLU(0.1)\n",
    "\n",
    "        )\n",
    "    def forward(self,x,train):\n",
    "        if train:\n",
    "            x = self.noise(x)\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = F.avg_pool2d(x, x.size()[2:]).squeeze()\n",
    "        x = self.linear(x)\n",
    "        return x "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 학습 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = {}\n",
    "cfg['dataset'] = 'cifar10'\n",
    "cfg['model_name'] = 'resnet18'\n",
    "cfg['unlabel_ratio'] = 0.6\n",
    "cfg['batch_size'] = 100 \n",
    "cfg['device'] = 'cuda:0'\n",
    "cfg['lr'] = 0.003 \n",
    "cfg['beta1'] = 0.8\n",
    "cfg['beta2'] = 0.999 \n",
    "cfg['epochs'] = 300 \n",
    "cfg['std'] = 0.15 \n",
    "cfg['super_only'] = False\n",
    "\n",
    "\n",
    "\n",
    "train_set,test_set = label_unlabel_load(cfg)\n",
    "valid_set = make_valid()\n",
    "\n",
    "train_dataset = CifarDataset(train_set, unlabel=False)\n",
    "valid_dataset = CifarDataset(valid_set, unlabel=False)\n",
    "test_dataset  = CifarDataset(test_set,unlabel=False)\n",
    "\n",
    "train_loader = DataLoader(train_dataset,batch_size=cfg['batch_size'],shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset,batch_size=cfg['batch_size'],shuffle=False)\n",
    "test_loader  = DataLoader(test_dataset,batch_size=cfg['batch_size'],shuffle=False)\n",
    "\n",
    "transformer = make_transform()\n",
    "\n",
    "#model = PiModel(device='cuda')\n",
    "model = Model(cfg['model_name']).to('cuda')\n",
    "criterion = PiCriterion(cfg)\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=cfg['lr'],betas=(cfg['beta1'],cfg['beta2']))\n",
    "\n",
    "\n",
    "\n",
    "best_epoch = np.inf \n",
    "for epoch in range(cfg['epochs']):\n",
    "    loss,tl_loss,tu_loss,weight =  train(model,criterion,optimizer,train_loader,cfg,transformer)\n",
    "    f1 , auc = valid(model,valid_loader,cfg)\n",
    "    print(f'\\n Epochs : {epoch}')\n",
    "    print(f'\\n loss : {loss} | tl_loss : {tl_loss} | tu_loss : {tu_loss}')\n",
    "    print(f'\\n valid f1 : {f1}')\n",
    "    print(f'\\n valid auc : {auc}')\n",
    "    \n",
    "    if loss < best_epoch:\n",
    "        torch.save(model,'./Save_models/best.pt')\n",
    "        best_epoch = loss \n",
    "        print(f'model saved | best loss :{best_epoch}')\n",
    "    '''\n",
    "    wandb.log({'loss':loss,\n",
    "               'tl_loss':tl_loss,\n",
    "               'tu_loss':tu_loss,\n",
    "               'weight':weight\n",
    "               })\n",
    "    '''\n",
    "    #if  loss > 10000:\n",
    "     #   model = torch.load('./Save_models/best.pt')\n",
    "      #  print('Model reloaded')\n",
    "f1 , auc = valid(model,test_loader,transformer,cfg)\n",
    "print(f\"\\n F1 score : {f1} | Auccuracy :\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.10 (default, Mar 15 2022, 12:22:08) \n[GCC 9.4.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
